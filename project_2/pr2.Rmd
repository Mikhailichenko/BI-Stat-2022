---
title: "Project â„– 2"
author: "Mikhailichenko Anastasia Sergeevna"
date: "2022-12-09"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, dpi = 300, warning=FALSE, message=FALSE)
if (!requireNamespace("ggplot2", quietly = TRUE))
  install.packages("ggplot2")
if (!requireNamespace("cowplot", quietly = TRUE))
  install.packages("cowplot")
if (!requireNamespace("MASS", quietly = TRUE))
  install.packages("MASS")
if (!requireNamespace("car", quietly = TRUE))
  install.packages("car")
if (!requireNamespace("gridExtra", quietly = TRUE))
  install.packages("gridExtra")
library(MASS)
library(ggplot2)
library(cowplot)
library(car)
library(gridExtra)
theme_set(theme_bw())
```





Download the dataset and see the beginning
```{r}
data("Boston")

head(Boston)
```
Check if there are empty values
```{r}
colSums(is.na(Boston))
```
They are no NA here. Hooray!

Standardize the predictors and write them into a new variable. Do not touch discrete variables (chas)
```{r}
Bostonst <- data.frame(scale(Boston[1:3], center=TRUE, scale = TRUE),
                       Boston[4],
                       scale(Boston[5:13], center=TRUE, scale = TRUE),
                       Boston[14])
```

Building a complete model
```{r}
mod <- lm(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat, data = Bostonst)
summary(mod)
```

Build Cleveland charts to search for pop-up values

```{r}

gg_dot <- ggplot(Bostonst, aes(y = 1:nrow(Bostonst))) + geom_point() + ylab('index')
Pl1 <- gg_dot + aes(x = crim)
Pl2 <- gg_dot + aes(x = zn)
Pl3 <- gg_dot + aes(x = indus)
Pl4 <- gg_dot + aes(x = chas)
Pl5 <- gg_dot + aes(x = nox)
Pl6 <- gg_dot + aes(x = rm)
Pl7 <- gg_dot + aes(x = age)
Pl8 <- gg_dot + aes(x = dis)
Pl9 <- gg_dot + aes(x = rad)
Pl10 <- gg_dot + aes(x = tax)
Pl11 <- gg_dot + aes(x = ptratio)
Pl12 <- gg_dot + aes(x = black)
Pl13 <- gg_dot + aes(x = lstat)


plot_grid(Pl1, Pl2, Pl3, Pl4, Pl5, Pl6,
          Pl7,Pl8, Pl9, Pl10, Pl11, Pl12, Pl13,
          ncol = 3, nrow = 5)

```

And plot Cook's distance chart
```{r}

mod_diag <- data.frame(fortify(mod))

ggplot(data = mod_diag, aes(x = 1:nrow(mod_diag), y = .cooksd)) + 
  geom_bar(stat = "identity") + 
  geom_hline(yintercept = 4/nrow(mod_diag), colour ='blue') 


```
Outliers are already visible on the Cleveland charts, and the Cooke distance plot only confirms this, as there are a number of observations above the threshold. They are too powerful in this model, and when refining the model, it is better to exclude them from the analysis.

Build a graph of the residuals. There is a suspicious straight line, it may have arisen due to outlayers
```{r}
gg_resid <- ggplot(data = mod_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + geom_hline(yintercept = 0)
gg_resid

```

In the case of some graphs of the dependence of residuals on predictors, non-random patterns are visible, hinting at the nonlinearity of relationships (pm, lstat) and heterogeneity (age, dis)
```{r}
res_1 <- gg_resid + aes(x = crim)
res_2 <- gg_resid + aes(x = zn)
res_3 <- gg_resid + aes(x = indus)
res_4 <- gg_resid + aes(x = chas)
res_5 <- gg_resid + aes(x = nox)
res_6 <- gg_resid + aes(x = rm)
res_7 <- gg_resid + aes(x = age)
res_8 <- gg_resid + aes(x = dis)
res_9 <- gg_resid + aes(x = rad)
res_10 <- gg_resid + aes(x = tax)
res_11 <- gg_resid + aes(x = ptratio)
res_12 <- gg_resid + aes(x = black)
res_13 <- gg_resid + aes(x = lstat)


grid.arrange(res_1, res_2, res_3, res_4,
             res_5, res_6, res_7, res_8, res_9, res_10,
             res_11, res_12, res_13, nrow = 5)

```

On the qq-plot, we see significant deviations from the normal distribution
```{r}
qqPlot(mod)
```

Pair plots show notable correlations between predictors
```{r}
pairs(Bostonst)
```
The full linear model does not satisfy any conditions for the applicability of a linear model, so dividing predictions by it is not a good idea.


Still, let's build a graph of the dependence of the variable response on the variable that has the largest coefficient (lstat) in absolute value.
```{r}
ggplot(Bostonst, aes(x = lstat, y = medv)) + 
  geom_point() + 
  geom_smooth(method="lm")
```

Let's try to improve the model a bit. We remove the multicollinearity of predictors by evaluating vif. One by one, we will exclude predictors in the largest vif from the model until all vif are less than 3.
```{r}
vif(mod)
mod <- update(mod, . ~ . - tax)
vif(mod)
mod <- update(mod, . ~ . - nox)
vif(mod)
mod <- update(mod, . ~ . - dis)
vif(mod)
```

```{r}
summary(mod)
```

Now we will get rid of insignificant predictors one by one and we will exclude the least significant ones until only those for whom the effect on the response variable is statistically significant remain.
```{r}
drop1(mod, test = "F")
mod <- update(mod, . ~ . - zn)
drop1(mod, test = "F")
mod <- update(mod, . ~ . - indus)
drop1(mod, test = "F")
mod <- update(mod, . ~ . - age)
drop1(mod, test = "F")
```

```{r}
summary(mod)
```
The improved model is not perfect, but if you start from it, the developer should look at areas near the Charles River, with a high index of accessibility to radial highways, with a high percentage of black residents, and with a high average number of rooms per dwelling. It is worth avoiding areas with a high lower status of the population, with a high crime rate and a high student-to-teacher ratio in the city.

